version: "3.9"

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.2.2
    container_name: es01
    environment:
      - node.name=es01
      - discovery.type=single-node
      # Dev-mode: security/TLS disabled. If you already have a secured cluster,
      # ignore this service and point ES_URL in .env at your existing ES.
      #- xpack.security.enabled=false
      #- xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - wids

  kibana:
    image: docker.elastic.co/kibana/kibana:9.2.2
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://es01:9200
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    networks:
      - wids

  # Optional: Elastic's MCP server container (you may not need this in 9.2+ if using Kibana's MCP endpoint)
  mcp:
    image: docker.elastic.co/elastic/mcp-server:latest
    container_name: elastic-mcp
    environment:
      - MCP_ELASTICSEARCH_URL=http://es01:9200
      - MCP_ELASTICSEARCH_USERNAME=${ES_USERNAME:-elastic}
      - MCP_ELASTICSEARCH_PASSWORD=${ES_PASSWORD:-changeme}
      - MCP_ELASTICSEARCH_VERIFY_CERTS=false
      # Example local LLM config (placeholder):
      # - MCP_LLM_PROVIDER=ollama
      # - MCP_LLM_ENDPOINT=http://ollama:11434
    depends_on:
      - elasticsearch
    networks:
      - wids

  # Local LLM for cost-effective enrichment
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - wids
    restart: unless-stopped

  feature-extractor:
    build:
      context: ./feature_extractor
      dockerfile: Dockerfile
    image: wids-feature-extractor
    container_name: wids-feature-extractor
    env_file:
      - .env
    depends_on:
      - elasticsearch
    networks:
      - wids
    # Uncomment for long-running daemon mode
    restart: unless-stopped
    
  kismet:
    build:
      context: ./kismet
      dockerfile: Dockerfile
    image: kismet
    container_name: kismet
    networks:
      - wids
    # Uncomment for long-running daemon mode
    # restart: unless-stopped
    
  # Autonomous enrichment worker (ES -> Ollama -> ES)
  context-enricher:
    build:
      context: ./context_enricher
      dockerfile: Dockerfile
    image: wids-context-enricher
    container_name: wids-context-enricher
    env_file:
      - .env
    environment:
      # Ensure the worker can reach Ollama on the compose network
      - OLLAMA_URL=http://ollama:11434
      # Set to match whatever you pulled in Ollama (example)
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1}
      # Polling behavior
      - POLL_SECONDS=${POLL_SECONDS:-10}
      - BATCH_SIZE=${BATCH_SIZE:-10}
      - TIME_WINDOW=${TIME_WINDOW:-now-24h}
      - WRITE_STRUCTURED_CONTEXT=${WRITE_STRUCTURED_CONTEXT:-true}
    depends_on:
      - elasticsearch
      - ollama
    networks:
      - wids
    restart: unless-stopped

  ml-trainer:
    build:
      context: ./ml
      dockerfile: Dockerfile
    image: wids-train-ml
    container_name: wids-train-ml
    env_file:
      - .env
    depends_on:
      - elasticsearch
    networks:
      - wids
    volumes:
      - ./ml/ml_output:/app/ml_output
    # This is intended as a one-shot job; run with:
    #   docker compose run --rm ml-trainer

networks:
  wids:
    driver: bridge

volumes:
  esdata:
  ollama:
